{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Cài đặt lại đúng phiên bản PyTorch và CUDA phù hợp trên Colab\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip uninstall -y nvidia-cublas-cu12 nvidia-cudnn-cu12 nvidia-cuda-runtime-cu12 nvidia-cuda-cupti-cu12 nvidia-cuda-nvrtc-cu12 nvidia-cufft-cu12 nvidia-curand-cu12 nvidia-cusolver-cu12 nvidia-cusparse-cu12 nvidia-nvjitlink-cu12\n",
        "\n",
        "# Cài đúng bản torch + CUDA 12.4\n",
        "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu124\n",
        "\n",
        "# Cài đúng các thư viện NVIDIA CUDA phụ trợ yêu cầu bởi torch 2.6.0+cu124\n",
        "!pip install \\\n",
        "  nvidia-cublas-cu12==12.4.5.8 \\\n",
        "  nvidia-cudnn-cu12==9.1.0.70 \\\n",
        "  nvidia-cuda-runtime-cu12==12.4.127 \\\n",
        "  nvidia-cuda-cupti-cu12==12.4.127 \\\n",
        "  nvidia-cuda-nvrtc-cu12==12.4.127 \\\n",
        "  nvidia-cufft-cu12==11.2.1.3 \\\n",
        "  nvidia-curand-cu12==10.3.5.147 \\\n",
        "  nvidia-cusolver-cu12==11.6.1.9 \\\n",
        "  nvidia-cusparse-cu12==12.3.1.170 \\\n",
        "  nvidia-nvjitlink-cu12==12.4.127\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnkWWvjzdzjv",
        "outputId": "c098a9e1-f8a1-45ba-ce8c-210958f4bdd5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.7.0.dev20250226+cu124\n",
            "Uninstalling torch-2.7.0.dev20250226+cu124:\n",
            "  Successfully uninstalled torch-2.7.0.dev20250226+cu124\n",
            "Found existing installation: torchvision 0.22.0.dev20250226+cu124\n",
            "Uninstalling torchvision-0.22.0.dev20250226+cu124:\n",
            "  Successfully uninstalled torchvision-0.22.0.dev20250226+cu124\n",
            "Found existing installation: torchaudio 2.6.0.dev20250226+cu124\n",
            "Uninstalling torchaudio-2.6.0.dev20250226+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0.dev20250226+cu124\n",
            "Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "  Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "  Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "  Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "  Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "  Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "  Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "  Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "  Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "  Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "  Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu124\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/torch-2.7.0.dev20250310%2Bcu124-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/torchvision-0.22.0.dev20250226%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting torchaudio\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/torchaudio-2.6.0.dev20250226%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pytorch-triton==3.2.0+git4b3bb1f8 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0+git4b3bb1f8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/nightly/cu124/torch-2.7.0.dev20250226%2Bcu124-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached https://download.pytorch.org/whl/nightly/cu124/torchvision-0.22.0.dev20250226%2Bcu124-cp311-cp311-linux_x86_64.whl (7.5 MB)\n",
            "Using cached https://download.pytorch.org/whl/nightly/cu124/torch-2.7.0.dev20250226%2Bcu124-cp311-cp311-manylinux_2_28_x86_64.whl (864.6 MB)\n",
            "Using cached https://download.pytorch.org/whl/nightly/cu124/torchaudio-2.6.0.dev20250226%2Bcu124-cp311-cp311-linux_x86_64.whl (3.5 MB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.7.0.dev20250226+cu124 torchaudio-2.6.0.dev20250226+cu124 torchvision-0.22.0.dev20250226+cu124\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (12.4.127)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (12.4.127)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall -y fastai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCh0j8NCfteu",
        "outputId": "7f558da0-abbc-4165-a361-a0b0b15ed093"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping fastai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "l11UHHWxNr4q"
      },
      "outputs": [],
      "source": [
        "!pip install -qq datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BLCqBtsKhz9-"
      },
      "outputs": [],
      "source": [
        "!pip show torch -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dgPVNiCN_7O",
        "outputId": "3b2022fa-362a-42b4-e05b-3d49ea5cc569"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DuwYRftQjQmQcAR4FVPH-HvGuxGi4ist\n",
            "To: /content/train_word.conll\n",
            "100% 1.42M/1.42M [00:00<00:00, 43.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11xZZfla8CDH54-EeUUdnAAoT2ummuEJh\n",
            "To: /content/test_word.conll\n",
            "100% 958k/958k [00:00<00:00, 78.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wVyhQkhAzwod2at7Ir3tRHbdCMOszzwg\n",
            "To: /content/dev_word.conll\n",
            "100% 628k/628k [00:00<00:00, 93.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1DuwYRftQjQmQcAR4FVPH-HvGuxGi4ist\n",
        "!gdown 11xZZfla8CDH54-EeUUdnAAoT2ummuEJh\n",
        "!gdown 1wVyhQkhAzwod2at7Ir3tRHbdCMOszzwg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ycmHHdkmN8q"
      },
      "source": [
        "Định nghĩa hàm read_conll() để đọc dữ liệu NER từ file .conll\n",
        "\n",
        "Trích xuất từng câu (sentences) và nhãn thực thể tương ứng (labels)\n",
        "\n",
        "Lưu trữ nhãn duy nhất (unique_labels) để phục vụ huấn luyện\n",
        "\n",
        "Đọc 3 bộ dữ liệu: train, dev, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HThNz_SfOW47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab9da46-e839-4234-c9dd-e43dfe2888a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels found: {'I-SYMPTOM_AND_DISEASE', 'I-ORGANIZATION', 'B-JOB', 'B-DATE', 'B-NAME', 'I-JOB', 'B-TRANSPORTATION', 'B-GENDER', 'O', 'I-PATIENT_ID', 'I-AGE', 'I-NAME', 'I-DATE', 'I-TRANSPORTATION', 'B-AGE', 'I-LOCATION', 'B-SYMPTOM_AND_DISEASE', 'B-LOCATION', 'B-ORGANIZATION', 'B-PATIENT_ID'}\n",
            "Unique labels found: {'I-SYMPTOM_AND_DISEASE', 'I-ORGANIZATION', 'B-JOB', 'B-DATE', 'B-NAME', 'I-JOB', 'B-TRANSPORTATION', 'B-GENDER', 'O', 'I-PATIENT_ID', 'I-NAME', 'I-DATE', 'I-TRANSPORTATION', 'B-AGE', 'I-LOCATION', 'B-SYMPTOM_AND_DISEASE', 'B-LOCATION', 'B-ORGANIZATION', 'B-PATIENT_ID'}\n",
            "Unique labels found: {'I-SYMPTOM_AND_DISEASE', 'I-ORGANIZATION', 'B-JOB', 'B-DATE', 'B-NAME', 'I-JOB', 'B-TRANSPORTATION', 'B-GENDER', 'O', 'I-PATIENT_ID', 'I-AGE', 'I-NAME', 'I-DATE', 'I-TRANSPORTATION', 'B-AGE', 'I-LOCATION', 'B-SYMPTOM_AND_DISEASE', 'B-LOCATION', 'B-ORGANIZATION', 'B-PATIENT_ID'}\n"
          ]
        }
      ],
      "source": [
        "# để đọc file định dạng CoNLL\n",
        "def read_conll(file_path):\n",
        "    sentences = []\n",
        "    sentence_labels = []\n",
        "    unique_labels = set()  # To collect unique labels\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        current_sentence_tokens = []\n",
        "        current_sentence_labels = []\n",
        "\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove leading/trailing whitespace, including '\\n'\n",
        "\n",
        "            # If it's an empty line, sentence boundary detected\n",
        "            if not line:\n",
        "                if current_sentence_tokens:  # Check if there's a sentence to append\n",
        "                    sentences.append(' '.join(current_sentence_tokens))\n",
        "                    sentence_labels.append(' '.join(current_sentence_labels))\n",
        "                current_sentence_tokens = []  # Reset for the next sentence\n",
        "                current_sentence_labels = []  # Reset for the next sentence\n",
        "            else:\n",
        "                line_parts = line.split()  # Split line into token and label\n",
        "                current_sentence_tokens.append(line_parts[0])\n",
        "\n",
        "                if len(line_parts) >= 2:\n",
        "                    current_sentence_labels.append(line_parts[1])\n",
        "                    unique_labels.add(line_parts[1])  # Add label to the set of unique labels\n",
        "                else:\n",
        "                    current_sentence_labels.append('O')  # Default to 'O' if no label provided\n",
        "\n",
        "    # Append the last sentence if the file doesn't end with an empty line\n",
        "    if current_sentence_tokens:\n",
        "        sentences.append(' '.join(current_sentence_tokens))\n",
        "        sentence_labels.append(' '.join(current_sentence_labels))\n",
        "\n",
        "    print(f\"Unique labels found: {unique_labels}\")\n",
        "    return sentences, sentence_labels\n",
        "\n",
        "# Load the datasets\n",
        "test_sentences, test_labels = read_conll('./test_word.conll')\n",
        "dev_sentences, dev_labels = read_conll('./dev_word.conll')\n",
        "train_sentences, train_labels = read_conll('./train_word.conll')\n",
        "\n",
        "# Now, test_sentences, test_labels, dev_sentences, dev_labels, train_sentences, and train_labels are arrays of strings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "etOibUB5Oal5",
        "outputId": "ddcad365-0d05-4c4b-d9fd-7e6a174b6e05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bác_sĩ Trần_Thanh_Linh , từ Bệnh_viện Chợ_Rẫy chi_viện phụ_trách đơn_nguyên hồi_sức tích_cực , cho biết \" bệnh_nhân 416 \" vẫn đang duy_trì ECMO , thở máy , hiện xơ phổi rất nhiều .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "test_sentences[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "fBlMKKr9OdFV",
        "outputId": "ae5bc07c-f387-482c-9cc3-e159cfc6900e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'O O O O B-ORGANIZATION I-ORGANIZATION O O O O O O O O O O B-PATIENT_ID O O O O O O O O O O B-SYMPTOM_AND_DISEASE I-SYMPTOM_AND_DISEASE I-SYMPTOM_AND_DISEASE I-SYMPTOM_AND_DISEASE O'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_labels[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWEiHV8zOiPr",
        "outputId": "dd4b3899-660b-490e-82f2-6e87bdf1333b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 5027\n",
            "Dev dataset size: 2000\n",
            "Test dataset size: 3000\n",
            "Train dataset sample: {'tokens': ['Đồng_thời', ',', 'bệnh_viện', 'tiếp_tục', 'thực_hiện', 'các', 'biện_pháp', 'phòng_chống', 'dịch_bệnh', 'COVID', '-', '19', 'theo', 'hướng_dẫn', 'của', 'Bộ', 'Y_tế', '.'], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O']}\n",
            "Dev dataset sample: {'tokens': ['Bác_sĩ', 'Nguyễn_Trung_Nguyên', ',', 'Giám_đốc', 'Trung_tâm', 'Chống', 'độc', ',', 'Bệnh_viện', 'Bạch_Mai', ',', 'cho', 'biết', 'bệnh_nhân', 'được', 'chuyển', 'đến', 'bệnh_viện', 'ngày', '7/3', ',', 'chẩn_đoán', 'ngộ_độc', 'thuốc', 'điều_trị', 'sốt_rét', 'chloroquine', '.'], 'labels': ['O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'I-ORGANIZATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DATE', 'O', 'O', 'B-SYMPTOM_AND_DISEASE', 'I-SYMPTOM_AND_DISEASE', 'O', 'O', 'O', 'O']}\n",
            "Test dataset sample: {'tokens': ['Từ', '24', '-', '7', 'đến', '31', '-', '7', ',', 'bệnh_nhân', 'được', 'mẹ', 'là', 'bà', 'H.T.P', '(', '47', 'tuổi', ')', 'đón', 'về', 'nhà', 'ở', 'phường', 'Phước_Hoà', '(', 'bằng', 'xe_máy', ')', ',', 'không', 'đi', 'đâu', 'chỉ', 'ra', 'Tạp_hoá', 'Phượng', ',', 'chợ', 'Vườn_Lài', ',', 'phường', 'An_Sơn', 'cùng', 'mẹ', 'bán', 'tạp_hoá', 'ở', 'đây', '.'], 'labels': ['O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'B-DATE', 'I-DATE', 'I-DATE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME', 'O', 'B-AGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'B-LOCATION', 'I-LOCATION', 'O', 'O', 'B-JOB', 'I-JOB', 'O', 'O', 'O']}\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Step 1: Prepare the datasets from sentences and labels\n",
        "def prepare_dataset(sentences, labels):\n",
        "    return {'tokens': sentences, 'labels': labels}\n",
        "\n",
        "train_dataset = prepare_dataset(train_sentences, train_labels)\n",
        "dev_dataset = prepare_dataset(dev_sentences, dev_labels)\n",
        "test_dataset = prepare_dataset(test_sentences, test_labels)\n",
        "\n",
        "# Step 2: Convert strings of tokens and labels into arrays\n",
        "def process_string_to_array(dataset):\n",
        "    return {\n",
        "        'tokens': [sentence.split() for sentence in dataset['tokens']],\n",
        "        'labels': [label_seq.split() for label_seq in dataset['labels']]\n",
        "    }\n",
        "\n",
        "# Step 3: Process the dataset for token and label lists\n",
        "train_dataset = process_string_to_array(train_dataset)\n",
        "dev_dataset = process_string_to_array(dev_dataset)\n",
        "test_dataset = process_string_to_array(test_dataset)\n",
        "\n",
        "# Step 4: Convert processed datasets into Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_dataset)\n",
        "dev_dataset = Dataset.from_dict(dev_dataset)\n",
        "test_dataset = Dataset.from_dict(test_dataset)\n",
        "\n",
        "# Print the size of each dataset and a sample for verification\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Dev dataset size: {len(dev_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Train dataset sample:\", train_dataset[0])\n",
        "print(\"Dev dataset sample:\", dev_dataset[0])\n",
        "print(\"Test dataset sample:\", test_dataset[0])\n",
        "\n",
        "# Step 5: Define an Example class\n",
        "class Example:\n",
        "    def __init__(self, words, slot_labels, guid=None):\n",
        "        self.words = words\n",
        "        self.slot_labels = slot_labels\n",
        "        self.guid = guid\n",
        "\n",
        "# Step 6: Convert the dataset to Example objects\n",
        "def convert_to_examples(dataset):\n",
        "    return [\n",
        "        Example(words=tokens, slot_labels=labels, guid=i)\n",
        "        for i, (tokens, labels) in enumerate(zip(dataset['tokens'], dataset['labels']))\n",
        "    ]\n",
        "\n",
        "# Convert datasets into Example objects\n",
        "train_examples = convert_to_examples(train_dataset)\n",
        "dev_examples = convert_to_examples(dev_dataset)\n",
        "test_examples = convert_to_examples(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EruR7doqOmFt"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wLI6cTIvOo6a"
      },
      "outputs": [],
      "source": [
        "def convert_examples_to_features(\n",
        "    examples,\n",
        "    max_seq_len,\n",
        "    tokenizer,\n",
        "    pad_label_id=-100,\n",
        "    cls_token_segment_id=0,\n",
        "    pad_token_segment_id=0,\n",
        "    sequence_segment_id=0,\n",
        "    mask_padding_with_zero=True,\n",
        "):\n",
        "    # Get special tokens from the tokenizer\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    unk_token = tokenizer.unk_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # List to hold the converted features\n",
        "    features = []\n",
        "\n",
        "    for example_index, example in enumerate(examples):\n",
        "        # Log progress every 5000 examples\n",
        "        if example_index % 400 == 0:\n",
        "            logger.info(f\"Processing example {example_index} of {len(examples)}\")\n",
        "\n",
        "        # Tokenize each word and align its corresponding label\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "\n",
        "        for word, label in zip(example.words, example.slot_labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "\n",
        "            # If the word cannot be tokenized, use [UNK] token\n",
        "            if not word_tokens:\n",
        "                word_tokens = [unk_token]\n",
        "\n",
        "            tokens.extend(word_tokens)\n",
        "\n",
        "            # Map string label to integer ID, apply pad_label_id for subword tokens\n",
        "            label_id = label_map[label]\n",
        "            label_ids.extend([label_id] + [pad_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        # Handle sequence truncation for [CLS] and [SEP] tokens\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:max_seq_len - special_tokens_count]\n",
        "            label_ids = label_ids[:max_seq_len - special_tokens_count]\n",
        "\n",
        "        # Add [SEP] token at the end of the sentence\n",
        "        tokens.append(sep_token)\n",
        "        label_ids.append(pad_label_id)\n",
        "        token_type_ids = [sequence_segment_id] * len(tokens)\n",
        "\n",
        "        # Add [CLS] token at the start of the sentence\n",
        "        tokens = [cls_token] + tokens\n",
        "        label_ids = [pad_label_id] + label_ids\n",
        "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "        # Convert tokens to input IDs\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # Create attention masks (1 for real tokens, 0 for padding tokens)\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "\n",
        "        # Pad sequences to the maximum sequence length\n",
        "        padding_length = max_seq_len - len(input_ids)\n",
        "        input_ids += [pad_token_id] * padding_length\n",
        "        attention_mask += [0 if mask_padding_with_zero else 1] * padding_length\n",
        "        token_type_ids += [pad_token_segment_id] * padding_length\n",
        "        label_ids += [pad_label_id] * padding_length\n",
        "\n",
        "        # Create InputFeatures object and append it to the list of features\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "                slot_labels_ids=label_ids,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dsBV-cAMOrdj"
      },
      "outputs": [],
      "source": [
        "# Define the label list (ensure that it includes all labels from your dataset)\n",
        "label_list = ['B-ORGANIZATION', 'B-TRANSPORTATION', 'B-JOB', 'I-PATIENT_ID', 'B-NAME', 'I-DATE', 'O', 'B-PATIENT_ID', 'I-AGE', 'I-JOB', 'B-DATE', 'I-TRANSPORTATION', 'B-SYMPTOM_AND_DISEASE', 'I-SYMPTOM_AND_DISEASE', 'B-GENDER', 'I-NAME', 'B-LOCATION', 'I-LOCATION', 'I-ORGANIZATION', 'B-AGE']\n",
        "\n",
        "# Create a mapping from label strings to integers\n",
        "label_map = {label: i for i, label in enumerate(label_list)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FF8qw0LAOtpD"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MzvngzMmOvky"
      },
      "outputs": [],
      "source": [
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, slot_labels_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.slot_labels_ids = slot_labels_ids\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPTuZi_nOx-4",
        "outputId": "5c628f56-bcde-4adb-e2bf-dcca9272e587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<s>', '</s>', '<unk>', 1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "!pip install -qq transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Use the 'xlm-roberta-base' tokenizer, which uses the desired special tokens\n",
        "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "tokenizer.cls_token, tokenizer.sep_token, tokenizer.unk_token, tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jmNGRrqOPUBO"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', add_prefix_space=True)\n",
        "\n",
        "# Set the maximum sequence length\n",
        "max_seq_len = 128  # You can adjust this based on your model/input\n",
        "\n",
        "# Convert examples to features\n",
        "train_features = convert_examples_to_features(train_examples, max_seq_len, tokenizer)\n",
        "dev_features = convert_examples_to_features(dev_examples, max_seq_len, tokenizer)\n",
        "test_features = convert_examples_to_features(test_examples, max_seq_len, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Qcinh8gWP1UR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Define a Dataset class to wrap the tokenized features for training\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(feature.input_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(feature.attention_mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(feature.token_type_ids, dtype=torch.long),\n",
        "            'labels': torch.tensor(feature.slot_labels_ids, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Convert tokenized features into PyTorch datasets\n",
        "train_dataset = NERDataset(train_features)\n",
        "dev_dataset = NERDataset(dev_features)\n",
        "test_dataset = NERDataset(test_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r7ULOeoWP5ly",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b6ba478-e539-472a-e61f-7ba3529f6cae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([    0,  4236, 16948,  1376,  2023,  9085,  2590,  1215,   212,  1376,\n",
              "          2023,    46,   118,  2156,   741,  1376,  2023,  6382,   282,   298,\n",
              "          1215,  6873,  1376,  2023,  6382,   282, 12369,  1376,  3070,  9470,\n",
              "           642,  1215,    90,  1376,  2023,  8210,   438,  3553,  1376,  2023,\n",
              "         15389,   438,  1215,  3592,  1376,  2023,  6382,   282,   740,  1526,\n",
              "           438,  4003,  1376,  2023,  6382,   282,  1215,  3792,  1526,   642,\n",
              "          7843,  3849, 14292,  2590,  1215,   611,  1376,  2023,  3602,  2590,\n",
              "           385,  1376,  2023, 13859,   611,  1215,   428,  1376,  2023,  6382,\n",
              "           282,   298,  6247, 43814,   111,   753,     5,   139,  1368,  8188,\n",
              "          7487,  1376,  2023,  3726,  2590,  1215,   417,  1376,  3070,  4958,\n",
              "           282,   740,  1376,  2023,  6248,   102,   163,  1376,  2023,    27,\n",
              "           854,  1215,    90,  1376,  3070,  9470,   479,     2,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'labels': tensor([-100,    6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100,    6,    6, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100,    6, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100,    6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "            6, -100, -100,    6, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "            6, -100, -100, -100, -100, -100, -100, -100, -100, -100,    6, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    6, -100,\n",
              "            6,    6,    6, -100,    6, -100, -100, -100, -100, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100,    6, -100, -100, -100, -100,    0, -100,\n",
              "         -100, -100,   18, -100, -100, -100, -100, -100,    6, -100, -100, -100,\n",
              "         -100, -100, -100, -100, -100, -100, -100, -100])}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "81eHszcmP8XU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e450c15-8ac5-496b-ad15-e61454364309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaForTokenClassification\n",
        "\n",
        "# Define the number of unique labels (ensure this matches your dataset's label set)\n",
        "num_labels = len(label_list)  # e.g., the number of unique labels such as O, B-ORG, etc.\n",
        "\n",
        "# Load the RoBERTa model for token classification\n",
        "model = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels=num_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s2R6SkYhQRJH"
      },
      "outputs": [],
      "source": [
        "!pip install seqeval -qq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "# Tự động đăng nhập wandb\n",
        "os.environ[\"WANDB_API_KEY\"] = \"fb80f73fcb020dd331c4509a5851a96be928f490\"  # Thêm mã API của bạn ở đây\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    true_preds, true_labels = [], []\n",
        "\n",
        "    for pred, label in zip(preds, labels):\n",
        "        tmp_preds, tmp_labels = [], []\n",
        "        for p_i, l_i in zip(pred, label):\n",
        "            if l_i != -100:\n",
        "                tmp_preds.append(label_list[p_i])\n",
        "                tmp_labels.append(label_list[l_i])\n",
        "        true_preds.append(tmp_preds)\n",
        "        true_labels.append(tmp_labels)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, true_preds),\n",
        "        \"recall\": recall_score(true_labels, true_preds),\n",
        "        \"f1\": f1_score(true_labels, true_preds),\n",
        "    }\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    do_train=True,\n",
        "    do_eval=False,  # Không dùng evaluation_strategy\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=[\"none\"],  # Tắt WandB\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Huấn luyện và đánh giá\n",
        "for epoch in range(int(training_args.num_train_epochs)):\n",
        "    print(f\"\\n==== Epoch {epoch + 1} ====\")\n",
        "    trainer.train(resume_from_checkpoint=None)\n",
        "    eval_metrics = trainer.evaluate()\n",
        "    print(eval_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x_qQ-WR4-lX1",
        "outputId": "2343d1ac-05df-4363-d1ba-6993a5c6fbc1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-41ff445ecb0f>:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Epoch 1 ====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [945/945 07:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.449800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.793800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.596800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.454400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.468600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.374100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.374400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.257700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.274400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.247100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.234100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.211100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.248200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.208700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.225900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.188900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.140700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.181800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.179000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.221200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.159800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.121800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.147200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.124000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.157300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.099700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.109000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.111800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.202900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.096900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.092300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.094500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.125100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.127400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.071800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.112400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.099400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.118500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.091400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.093200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.081800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.083800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.109100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.096200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.104800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.093600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.101000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.079200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.087500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.076100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.083000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.104400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.116500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.080700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.081300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.095600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.081300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.058200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.055000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.066700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.072000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.051900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.062900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.066200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.080300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.052400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.076200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.052800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.050800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.069400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.054100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.046000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.084700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.047800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.052200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.052500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.097200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.055600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.058400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.051800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.043000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.095200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.035900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.049700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.063900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.057600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.057500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='375' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 15:45]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.12403533607721329, 'eval_precision': 0.9022881880024737, 'eval_recall': 0.913731016126507, 'eval_f1': 0.9079735511474135, 'eval_runtime': 13.9029, 'eval_samples_per_second': 143.855, 'eval_steps_per_second': 8.991, 'epoch': 3.0}\n",
            "\n",
            "==== Epoch 2 ====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [945/945 07:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.061100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.093600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.078800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.062700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.087400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.079100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.079600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.062000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.071400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.078400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.085300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.067200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.088700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.086000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.082700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.079000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.061000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.065100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.083000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.046400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.095100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.117300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.074300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.051300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.064100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.067800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.063100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.053500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.048300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.049100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.039000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.096100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.036100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.043900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.060100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.064500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.031400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.049400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.048600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.049600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.054400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.057700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.046300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.041100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.065500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.064800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.033300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.051800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.049900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.064200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.033900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.050900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.047200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.043200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.051500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.057100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.033900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.041200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.042700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.046800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.029600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.020300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.035600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.034400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.030900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.040200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.024500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.049500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.018100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.019300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.047700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.028500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.039900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.024700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.022500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.052100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.020400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.024500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.026000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.028600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.034900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.034300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.016600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.016900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.038900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.036500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.021600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.12387196719646454, 'eval_precision': 0.917910447761194, 'eval_recall': 0.9243776420854861, 'eval_f1': 0.9211326936578516, 'eval_runtime': 14.0329, 'eval_samples_per_second': 142.523, 'eval_steps_per_second': 8.908, 'epoch': 3.0}\n",
            "\n",
            "==== Epoch 3 ====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='945' max='945' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [945/945 07:16, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.022600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.045300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.035900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.036900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.039900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.066100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.037100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.034600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.045500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.046700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.047200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.036900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.039500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.057400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.048200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.036200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.044100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.055300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.032900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.061800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.074300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.050200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.041200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.046900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.038000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.040100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.027500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.032200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.026600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.048900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.030400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.020400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.017900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.026200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.035800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.042500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.014200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.043900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.038500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.036800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.034500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.036700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.025600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.036100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.044400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.033800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.035400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.017200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.037700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.033300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.022700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.041900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.040300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.028200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.021500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.027300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.016800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.022900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.015900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.012300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.019900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.024400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.009400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.014000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.023200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.008700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.012200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.023100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>0.016700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.014900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>0.012000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>0.035800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>0.014500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>0.007100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.013500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>0.022600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>0.013900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>0.021500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>0.021800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.013300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>0.014800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>0.031700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>0.022600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>0.012700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.1338542103767395, 'eval_precision': 0.9220455254131588, 'eval_recall': 0.9259433223735714, 'eval_f1': 0.9239903132567768, 'eval_runtime': 13.9378, 'eval_samples_per_second': 143.494, 'eval_steps_per_second': 8.968, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oq4u7q8NQYzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655c9dba-ddb5-4b5b-8cc6-6b82f279cb24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./roberta-ner-finetuned/tokenizer_config.json',\n",
              " './roberta-ner-finetuned/special_tokens_map.json',\n",
              " './roberta-ner-finetuned/vocab.json',\n",
              " './roberta-ner-finetuned/merges.txt',\n",
              " './roberta-ner-finetuned/added_tokens.json',\n",
              " './roberta-ner-finetuned/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "trainer.save_model(\"./roberta-ner-finetuned\")\n",
        "tokenizer.save_pretrained(\"./roberta-ner-finetuned\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sau khi huấn luyện xong, đánh giá mô hình trên test set\n",
        "\n",
        "# Tạo dataset từ file test (dựa vào NERDataset bạn đã định nghĩa từ trước)\n",
        "# Assuming label_map is the correct mapping, otherwise replace with the correct variable\n",
        "label_to_id = label_map\n",
        "test_dataset = NERDataset(test_features) # Modify this line to use test_features\n",
        "\n",
        "# Đánh giá mô hình trên test set\n",
        "test_metrics = trainer.evaluate(test_dataset)\n",
        "\n",
        "# In kết quả\n",
        "print(\"\\n====== Đánh giá trên TEST set ======\")\n",
        "print(f\"Loss: {test_metrics['eval_loss']:.4f}\")\n",
        "print(f\"Precision: {test_metrics['eval_precision']:.4f}\")\n",
        "print(f\"Recall: {test_metrics['eval_recall']:.4f}\")\n",
        "print(f\"F1 Score: {test_metrics['eval_f1']:.4f}\")"
      ],
      "metadata": {
        "id": "8K45SIsnb5Fx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "15a3d34d-a00b-46e5-e832-a19140929c63"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='563' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 19:33]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Đánh giá trên TEST set ======\n",
            "Loss: 0.1698\n",
            "Precision: 0.9153\n",
            "Recall: 0.9180\n",
            "F1 Score: 0.9167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "# Lấy nhãn thực và nhãn dự đoán từ tập test\n",
        "predictions, labels, _ = trainer.predict(test_dataset)\n",
        "preds = predictions.argmax(-1)\n",
        "\n",
        "true_labels = []\n",
        "true_preds = []\n",
        "\n",
        "for pred, label in zip(preds, labels):\n",
        "    true_label = []\n",
        "    true_pred = []\n",
        "    for p_i, l_i in zip(pred, label):\n",
        "        if l_i != -100:\n",
        "            true_label.append(label_list[l_i])\n",
        "            true_pred.append(label_list[p_i])\n",
        "    true_labels.append(true_label)\n",
        "    true_preds.append(true_pred)\n",
        "\n",
        "# In báo cáo chi tiết\n",
        "print(\"\\n====== Báo cáo chi tiết theo từng thực thể ======\")\n",
        "print(classification_report(true_labels, true_preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "mbLx_AsGdwzv",
        "outputId": "7861cc30-a342-4393-8fdb-73ea8f46590b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== Báo cáo chi tiết theo từng thực thể ======\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "                AGE     0.9652    0.9633    0.9643       518\n",
            "               DATE     0.9790    0.9904    0.9847      1459\n",
            "             GENDER     0.9379    0.9704    0.9539       405\n",
            "                JOB     0.5779    0.5742    0.5761       155\n",
            "           LOCATION     0.9130    0.9232    0.9181      3671\n",
            "               NAME     0.8943    0.8527    0.8730       258\n",
            "       ORGANIZATION     0.8165    0.8246    0.8205       707\n",
            "         PATIENT_ID     0.9697    0.9814    0.9755      1665\n",
            "SYMPTOM_AND_DISEASE     0.8093    0.7598    0.7838       916\n",
            "     TRANSPORTATION     0.9535    0.9535    0.9535       172\n",
            "\n",
            "          micro avg     0.9153    0.9180    0.9167      9926\n",
            "          macro avg     0.8816    0.8793    0.8803      9926\n",
            "       weighted avg     0.9145    0.9180    0.9161      9926\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}